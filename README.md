# ğŸš€ Transformer from Scratch

<div align="center">

[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)](https://pytorch.org/)
[![Lightning](https://img.shields.io/badge/Lightning-792EE5?style=for-the-badge&logo=pytorchlightning&logoColor=white)](https://pytorch-lightning.readthedocs.io/)
[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

**A complete, production-ready implementation of the Transformer architecture from "Attention Is All You Need"**

*Built with PyTorch Lightning for scalable training and inference*

</div>

---

## âœ¨ What Makes This Special

ğŸ¯ **Complete Implementation** - Every component from the original paper, meticulously crafted  
âš¡ **Lightning Fast** - PyTorch Lightning integration for distributed training  
ğŸ§  **Production Ready** - Proper error handling, logging, and checkpointing  
ğŸ”§ **Modular Design** - Each component is independently testable and reusable  
ğŸ§ª **Independent Testing** - Run each module separately for debugging and learning  
ğŸ“š **Educational** - Clean, well-documented code perfect for learning  
ğŸ¨ **Modern Stack** - Uses GPT-2 tokenizer and state-of-the-art practices  
ğŸš€ **Multiple Architectures** - CrossAttention, DecoderOnly, and MoE implementations  
ğŸ“Š **Comprehensive Metrics** - BLEU, ROUGE, METEOR, and BERTScore evaluation  
ğŸ›ï¸ **Advanced Features** - Mixture of Experts with Top-K routing and sparse computation  

---

## ğŸ—ï¸ Architecture Deep Dive

### Core Components

| Component | Description | Key Features |
|-----------|-------------|--------------|
| **ğŸ”¤ TokenEmbedding** | Converts tokens to dense vectors | Scaling, padding handling, vocabulary mapping |
| **ğŸ“ PositionalEmbedding** | Adds position information | Sinusoidal & learned encodings, flexible max positions |
| **ğŸ¯ MultiHeadSelfAttention** | The heart of Transformers | Causal masking, cross-attention, scaled dot-product |
| **ğŸ§  PositionwiseFeedForward** | Non-linear transformations | GELU activation, configurable dimensions |
| **â• AddNorm** | Residual connections + normalization | Layer normalization, dropout, gradient flow |
| **ğŸ“¥ Encoder** | Processes input sequences | Stacked layers, self-attention, context building |
| **ğŸ“¤ Decoder** | Generates output sequences | Masked attention, cross-attention, autoregressive |
| **ğŸ›ï¸ MoE Components** | Mixture of Experts implementation | Top-K routing, sparse computation, expert specialization |
| **ğŸ”€ TopKRouter** | Expert selection mechanism | Dynamic routing, load balancing, efficient computation |

### Model Architectures

| Architecture | Description | Use Cases | Key Features |
|--------------|-------------|-----------|--------------|
| **ğŸ”„ CrossAttentionSeq2Seq** | Full encoder-decoder with cross-attention | Translation, summarization | Bidirectional encoding, cross-attention |
| **ğŸ“ DecoderOnly** | GPT-style autoregressive model | Text generation, completion | Causal masking, next-token prediction |
| **ğŸ›ï¸ DecoderOnlyMoE** | Decoder-only with Mixture of Experts | Large-scale text generation | Sparse activation, expert routing |

### Data Flow

```mermaid
graph TD
    A[Input Text] --> B[Tokenization]
    B --> C[Token Embedding]
    C --> D[Positional Encoding]
    D --> E[Encoder Stack]
    E --> F[Context Vectors]
    F --> G[Decoder Stack]
    G --> H[Output Logits]
    H --> I[Generated Text]
```

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/transformer-from-scratch.git
cd transformer-from-scratch

# Install dependencies
pip install torch pytorch-lightning transformers pandas numpy sacrebleu rouge_score bert_score nltk
```

### 2. Training

Choose from multiple model architectures:

#### CrossAttention Seq2Seq Model
```bash
# Train encoder-decoder with cross-attention
python Trainer.py
```

#### Decoder-Only Model (GPT-style)
```bash
# Train decoder-only autoregressive model
python DecoderOnlyTrainer.py
```

#### Decoder-Only with Mixture of Experts
```bash
# Train MoE model with expert routing
python DecoderMoETrainer.py
```

**Training Features:**
- ğŸ¯ **Automatic checkpointing** - Best model saved automatically
- ğŸ“Š **Real-time monitoring** - Loss tracking and validation metrics
- ğŸ”„ **GPU acceleration** - GPU support
- ğŸ“ˆ **Progress tracking** - Detailed logging and progress bars
- ğŸ›ï¸ **MoE Support** - Sparse computation with expert routing
- ğŸ“Š **Comprehensive Metrics** - BLEU, ROUGE, METEOR, BERTScore evaluation

### 3. Inference

Choose the appropriate inference script for your model:

#### CrossAttention Seq2Seq Model
```bash
# Generate text completions with encoder-decoder
python Inference.py
```

#### Decoder-Only Model
```bash
# Generate text with decoder-only model
python DecoderOnlyInference.py
```

#### Decoder-Only with MoE
```bash
# Generate text with MoE model
python DecoderMoEInference.py
```

**Inference Features:**
- ğŸ² **Greedy decoding** - Deterministic text generation
- âš¡ **Fast inference** - Optimized for production use
- ğŸ¯ **Flexible input** - Handle variable length sequences
- ğŸ”§ **Easy integration** - Simple API for your applications
- ğŸ›ï¸ **MoE Support** - Efficient expert routing during inference
- ğŸ“Š **Multiple Models** - Support for different architectures

### 4. Independent Module Testing

Each component can be run independently for testing and experimentation:

```bash
# Test individual components
python Embedding.py              # Test token & positional embeddings
python MultiHeadSelfAttention.py # Test attention mechanism
python FFN.py                    # Test feed-forward network
python AddNorm.py                # Test residual connections & normalization
python Encoder.py                # Test encoder stack
python Decoder.py                # Test decoder stack
python Seq2SeqModel.py           # Test complete model
```

**Independent Testing Features:**
- ğŸ§ª **Component isolation** - Test each part separately
- ğŸ” **Debugging friendly** - Easy to identify issues in specific components
- ğŸ“š **Learning focused** - Understand each component's behavior individually
- âš¡ **Quick validation** - Fast testing without full training pipeline

---

## ğŸ“Š Evaluation Metrics

The codebase includes comprehensive evaluation metrics for assessing model performance:

### Automatic Metrics

| Metric | Description | Range | Use Case |
|--------|-------------|-------|----------|
| **ğŸ¯ BLEU** | N-gram overlap with reference | 0-100 | Translation quality, text similarity |
| **ğŸ“ ROUGE-1** | Unigram overlap | 0-1 | Content coverage, summarization |
| **ğŸ“ ROUGE-2** | Bigram overlap | 0-1 | Phrase-level similarity |
| **ğŸ“ ROUGE-L** | Longest common subsequence | 0-1 | Structural similarity |
| **â˜„ï¸ METEOR** | Semantic similarity with synonyms | 0-1 | Meaning preservation |
| **ğŸ§  BERTScore** | Contextual embedding similarity | 0-1 | Semantic understanding |

### Implementation Features

- **ğŸ“Š Real-time Tracking** - Metrics computed during validation
- **ğŸ“ˆ Progress Monitoring** - All metrics logged to PyTorch Lightning
- **ğŸ”„ Automatic Evaluation** - No manual intervention required
- **âš¡ Efficient Computation** - Optimized for large-scale evaluation
- **ğŸ“‹ Comprehensive Coverage** - Multiple evaluation perspectives

### Usage

All metrics are automatically computed during training validation steps and logged to the progress bar and tensorboard logs.

---

## ğŸ“Š Dataset & Task

**Versatile Text Completion Dataset**
- ğŸ“ **2,000 examples** of diverse text completion pairs
- ğŸ¯ **Task**: Complete partial sentences with meaningful continuations
- ğŸ“ **Format**: `"partial sentence..." â†’ "completion text"`
- ğŸ”„ **Train/Val Split**: 80/20 automatic split
- ğŸŒ **Diverse Topics**: Covers multiple domains and contexts

**Example:**
```
Input:  "The rise of renewable energy is changing global markets and Experts predict this shift will redefine economies"
Output: "reducing dependence on fossil fuels and lowering emissions."
```

**Dataset Features:**
- ğŸ“š **Educational Content** - Science, technology, and general knowledge
- ğŸ”„ **Multiple Formats** - Various sentence structures and completion types
- ğŸ¯ **Quality Controlled** - Curated for meaningful learning objectives
- ğŸ“Š **Balanced Distribution** - Even representation across different topics

---

## âš™ï¸ Configuration

### Model Architecture

| Parameter | Default | Description |
|-----------|---------|-------------|
| `d_model` | 256 | Model dimension (embedding size) |
| `num_heads` | 4-8 | Number of attention heads |
| `num_encoder_layers` | 2-6 | Encoder stack depth |
| `num_decoder_layers` | 2-6 | Decoder stack depth |
| `d_ff` | 128-1024 | Feed-forward dimension |
| `dropout` | 0.1 | Dropout rate |
| `max_positions` | 32-512 | Maximum sequence length |
| `use_sinusoidal_pos` | True | Use sinusoidal positional encoding |

### MoE Configuration (DecoderOnlyMoE)

| Parameter | Default | Description |
|-----------|---------|-------------|
| `num_experts` | 4 | Number of expert networks |
| `top_k` | 2 | Number of experts to activate per token |
| `expert_capacity` | Auto | Maximum tokens per expert |

### Training Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| `batch_size` | 4 | Training batch size |
| `learning_rate` | 1e-3 | Adam optimizer learning rate |
| `max_epochs` | 100 | Maximum training epochs |
| `gradient_clip` | 1.0 | Gradient clipping threshold |
| `checkpoint_monitor` | val_loss_epoch | Model selection metric |

---

## ğŸ“ Project Structure

```
transformer-from-scratch/
â”œâ”€â”€ ğŸ§  Core Components
â”‚   â”œâ”€â”€ Embedding.py              # Token & positional embeddings
â”‚   â”œâ”€â”€ MultiHeadSelfAttention.py # Multi-head attention mechanism
â”‚   â”œâ”€â”€ FFN.py                    # Position-wise feed-forward
â”‚   â””â”€â”€ AddNorm.py                # Residual connections + normalization
â”œâ”€â”€ ğŸ—ï¸ Architecture Models
â”‚   â”œâ”€â”€ Encoder.py                # Encoder stack implementation
â”‚   â”œâ”€â”€ Decoder.py                # Decoder stack implementation
â”‚   â”œâ”€â”€ CrossAttentionSeq2SeqModel.py  # Full encoder-decoder model
â”‚   â”œâ”€â”€ DecoderOnlySeq2SeqModel.py     # GPT-style decoder-only model
â”‚   â””â”€â”€ DecoderMoE.py             # Decoder-only with Mixture of Experts
â”œâ”€â”€ ğŸš€ Training Scripts
â”‚   â”œâ”€â”€ Trainer.py                # CrossAttention training pipeline
â”‚   â”œâ”€â”€ DecoderOnlyTrainer.py     # Decoder-only training pipeline
â”‚   â””â”€â”€ DecoderMoETrainer.py      # MoE training pipeline
â”œâ”€â”€ ğŸ¯ Inference Scripts
â”‚   â”œâ”€â”€ Inference.py              # CrossAttention inference
â”‚   â”œâ”€â”€ DecoderOnlyInference.py   # Decoder-only inference
â”‚   â””â”€â”€ DecoderMoEInference.py    # MoE inference
â”œâ”€â”€ ğŸ“Š Data
â”‚   â”œâ”€â”€ versatile_dataset_2000.csv     # Main training dataset
â”‚   â””â”€â”€ synthetic_text_completion.csv  # Legacy dataset
â”œâ”€â”€ ğŸ“ Checkpoints
â”‚   â”œâ”€â”€ Seq2SeqCheckpoints/       # CrossAttention model checkpoints
â”‚   â”œâ”€â”€ DecoderOnlyCheckpoints/   # Decoder-only model checkpoints
â”‚   â””â”€â”€ DecoderMoECheckpoints/    # MoE model checkpoints
â””â”€â”€ ğŸ“ˆ Logs
    â””â”€â”€ lightning_logs/           # Training logs and metrics
```

---

## ğŸ¯ Use Cases

### Perfect For:
- ğŸ“š **Learning** - Understanding Transformer architecture
- ğŸ”¬ **Research** - Experimenting with attention mechanisms
- ğŸš€ **Prototyping** - Quick seq2seq model development
- ğŸ§ª **Component Testing** - Debug and validate individual modules

### Applications:

#### CrossAttention Seq2Seq Model
- ğŸ“„ **Summarization** - Generate concise summaries
- ğŸ”„ **Translation** - Sequence-to-sequence translation
- ğŸ“ **Question Answering** - Context-aware responses
- ğŸ“Š **Data-to-Text** - Convert structured data to natural language

#### Decoder-Only Models
- ğŸ“ **Text Completion** - Auto-complete sentences
- ğŸ’¬ **Chatbots** - Conversational AI systems
- ğŸ¨ **Creative Writing** - Story and content generation
- ğŸ” **Code Generation** - Programming assistance

#### MoE Models
- ğŸš€ **Large-Scale Generation** - Efficient text generation at scale
- ğŸ¯ **Specialized Tasks** - Expert routing for domain-specific content
- âš¡ **Resource Optimization** - Sparse computation for better efficiency
- ğŸ§  **Multi-Domain Learning** - Handle diverse topics with specialized experts

---

## ğŸ›ï¸ Mixture of Experts (MoE) Implementation

### Key Features

The MoE implementation includes several advanced features for efficient sparse computation:

#### Expert Architecture
- **ğŸ”§ ExpertMLP** - Individual expert networks with GELU activation
- **ğŸ¯ TopKRouter** - Intelligent routing mechanism for expert selection
- **âš¡ Sparse Computation** - Only activate selected experts per token
- **ğŸ“Š Load Balancing** - Automatic expert capacity management

#### Routing Strategy
- **ğŸ² Softmax Gating** - Probabilistic expert selection
- **ğŸ” Top-K Selection** - Activate only the most relevant experts
- **ğŸ“ˆ Dynamic Routing** - Adaptive expert selection based on input
- **âš–ï¸ Load Balancing** - Prevent expert overloading

#### Performance Optimizations
- **ğŸš€ Sparse Activation** - Reduce computational overhead
- **ğŸ’¾ Memory Efficient** - Only store active expert outputs
- **ğŸ”„ Batch Processing** - Efficient parallel expert computation
- **ğŸ“Š Gradient Flow** - Proper backpropagation through routing

### Usage Example

```python
# Initialize MoE model
model = DecoderOnlyMoEModel(
    vocab_size=vocab_size,
    d_model=256,
    num_experts=4,      # Number of expert networks
    top_k=2,           # Activate top 2 experts per token
    num_layers=6,
    tokenizer=tokenizer
)

# Training automatically handles expert routing
trainer.fit(model, train_loader, val_loader)
```

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. ğŸ´ **Fork** the repository
2. ğŸŒŸ **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ **Commit** your changes (`git commit -m 'Add AmazingFeature'`)
4. ğŸ“¤ **Push** to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ”„ **Open** a Pull Request

### Areas for Contribution:
- ğŸš€ **Performance optimizations**
- ğŸ§ª **Additional attention mechanisms**
- ğŸ“Š **More datasets and tasks**
- ğŸ“š **Documentation improvements**
- ğŸ› **Bug fixes and testing**

---

## ğŸ“š References & Learning

### Papers
1. **Vaswani, A., et al.** (2017). "Attention is all you need." *NeurIPS 2017*
2. **Devlin, J., et al.** (2018). "BERT: Pre-training of Deep Bidirectional Transformers." *NAACL 2019*

### Resources
- ğŸ“– [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- âš¡ [PyTorch Lightning Documentation](https://pytorch-lightning.readthedocs.io/)
- ğŸ“ [Attention Mechanism Explained](https://distill.pub/2016/augmented-rnns/)
- ğŸ”¥ [Transformer from Scratch](https://www.youtube.com/watch?v=ISNdQcPhsts)

---

<div align="center">

**â­ Star this repository if you found it helpful!**

Made with â¤ï¸ and lots of â˜•

[Report Bug](https://github.com/yourusername/transformer-from-scratch/issues) Â· [Request Feature](https://github.com/yourusername/transformer-from-scratch/issues) Â· [Documentation](https://github.com/yourusername/transformer-from-scratch/wiki)

</div>